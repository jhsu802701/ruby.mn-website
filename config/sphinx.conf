#
# Sphinx configuration file sample
#

#############################################################################
## data source definition
#############################################################################

source tcrbb_prod
{
	# data source type
	# for now, known types are 'mysql', 'pgsql' and 'xmlpipe'
	# MUST be defined
	type				= mysql

	strip_html			= 0
	index_html_attrs	=

	sql_host			= 
	sql_user			= root
	sql_db				= tcrbb_prod
	sql_pass			= 
	sql_port			= 3306	
	sql_sock			= /var/run/mysqld/mysqld.sock
	#
	# optional
	# usually '/var/lib/mysql/mysql.sock' on Linux
	# usually '/tmp/mysql.sock' on FreeBSD

	# pre-query, executed before the main fetch query
	# useful eg. to setup encoding or mark records
	# optional, default is empty
	#
	# sql_query_pre		= SET CHARACTER_SET_RESULTS=cp1251
	sql_query_pre		=

	# main document fetch query
	#
	# you can specify up to 32 (formally SPH_MAX_FIELDS in sphinx.h) fields;
	# all of the fields which are not document_id or attributes (see below)
	# will be full-text indexed
	#
	# document_id MUST be the very first field
	# document_id MUST be positive (non-zero, non-negative)
	# document_id MUST fit into 32 bits
	# document_id MUST be unique
	#
	# mandatory
	sql_query			= \
  SELECT id, subject, body, list_mails.from, list_mails.to  from list_mails

	# query range setup
	#
	# useful to avoid MyISAM table locks and big result sets
	# when indexing lots of data
	#
	# to use query ranges, you should
	# 1) provide a query to fetch min/max id (ie. id range) from data set;
	# 2) configure step size in which this range will be walked;
	# 3) use $start and $end macros somewhere in the main fetch query.
	#
	# 'sql_query_range' must return exactly two integer fields
	# in exactly min_id, max_id order
	#
	# 'sql_range_step' must be a positive integer
	# optional, default is 1024
	#
	# 'sql_query' must contain both '$start' and '$end' macros
	# if you are using query ranges (because it obviously would be an
	# error to index the whole table many times)
	#
	# note that the intervals specified by $start/$end do not
	# overlap, so you should NOT remove document ids which are exactly
	# equal to $start or $end in your query
	#
	# here's an example which will index 'documents' table
	# fetching (at most) one thousand entries at a time:
	#
	# sql_query_range		= SELECT MIN(id),MAX(id) FROM documents
	# sql_range_step		= 1000
	# sql_query			= \
	#	SELECT doc.id, doc.id AS group, doc.title, doc.data \
	#	FROM documents doc \
	#	WHERE id>=$start AND id<=$end


	# attribute columns
	#
	# attribute values MUST be positive (non-zero, non-negative) integers
	# attribute values MUST fit into 32 bits
	#
	# attributes are additional values associated with each document which
	# may be used to perform additional filtering and sorting during search.
	# attributes are NOT full-text indexed; they are stored in the full text
	# index as is.
	#
	# a good example would be a forum posts table. one might need to search
	# through 'title' and 'content' fields but to limit search to specific
	# values of 'author_id', or 'forum_id', or to sort by 'post_date', or to
	# group matches by 'thread_id', or to group posts by month of the
	# 'post_date' and provide statistics.
	#
	# this all can be achieved by specifying all the mentioned columns
	# (excluding 'title' and 'content' which are full-text fields) as
	# attributes and then using API calls to setup filtering, sorting,
	# and grouping.
	#
	# sql_group_column is used to declare integer attributes.
	#
	# sql_date_column is used to declare UNIX timestamp attributes.
	#
	# sql_str2ordinal_column is used to declare integer attributes which 
	# values are computed as ordinal numbers of corresponding column value
	# in sorted list of column values. WARNING, all such strings values
	# are going to be stored in RAM while indexing, and "C" locale will
	# be used when sorting!
	#
	# starting with 0.9.7, there may be multiple attribute columns specified.
	# here's an example for that mentioned posts table:
	#
	#

	# post-query, executed on the end of main fetch query
	#
	# note that indexing is NOT completed at the point when post-query
	# gets executed and might very well fail
	#
	# optional, default is empty
	sql_query_post		=

	# post-index-query, executed on succsefully completed indexing
	#
	# $maxid macro is the max document ID which was actually
	# fetched from the database
	#
	# optional, default is empty
	#
	# sql_query_post_index = REPLACE INTO counters ( id, val ) \
	#	VALUES ( 'max_indexed_id', $maxid )


	# document info query
	#
	# ONLY used by search utility to display document information
	# MUST be able to fetch document info by its id, therefore
	# MUST contain '$id' macro 
	#
	# optional, default is empty
	sql_query_info		= SELECT * FROM list_mails WHERE id=$id

}


#############################################################################
## index definition
#############################################################################

# local index example
#
# this is an index which is stored locally in the filesystem
#
# all indexing-time options (such as morphology and charsets)
# are configured per local index
index list_mails
{
	# which document source to index
	# at least one MUST be defined
	#
	# multiple sources MAY be specified; to do so, just add more
	# "source = NAME" lines. in this case, ALL the document IDs
	# in ALL the specified sources MUST be unique
	source			= tcrbb_prod

	# this is path and index file name without extension
	#
	# indexer will append different extensions to this path to
	# generate names for both permanent and temporary index files
	#
	# .tmp* files are temporary and can be safely removed
	# if indexer fails to remove them automatically
	#
	# .sp* files are fulltext index data files. specifically,
	# .spa contains attribute values attached to each document id
	# .spd contains doclists and hitlists
	# .sph contains index header (schema and other settings)
	# .spi contains wordlists
	#
	# MUST be defined
  path			= ./sphinx_data/listmail

	docinfo			= extern

	#
	# morphology		= none
	morphology		= stem_en
	# morphology		= stem_ru
	# morphology		= stem_enru
	# morphology		= soundex

	# stopwords file
	#
	# format is plain text in whatever encoding you use
	# optional, default is empty
	#
	# stopwords			= /opt/local/var/data/stopwords.txt
	stopwords			=

	# minimum word length
	#
	# only the words that are of this length and above will be indexed;
	# for example, if min_word_len is 4, "the" won't be indexed,
	# but "they" will be.
	#
	# default is 1, which (obviously) means to index everything
	min_word_len		= 1

	# charset encoding type
	#
	# known types are 'sbcs' (Single Byte CharSet) and 'utf-8'
	#
	# optional, default is sbcs
	charset_type		= sbcs

	min_prefix_len		= 0
	min_infix_len		= 0
}



#############################################################################
## indexer settings
#############################################################################

indexer
{
	mem_limit			= 64M
}

#############################################################################
## searchd settings
#############################################################################

searchd
{
	# IP address on which search daemon will bind and accept
	# incoming network requests
	#
	# optional, default is to listen on all addresses,
	# ie. address = 0.0.0.0
	#
	address				= 127.0.0.1
	# address				= 192.168.0.1


	# port on which search daemon will listen
	port				= 3312


	# log file
	# searchd run info is logged here
  log					= ./sphinx_data/log/searchd.log


	# query log file
	# all the search queries are logged here
  query_log			= ./sphinx_data/log/query.log


	# client read timeout, seconds
	read_timeout		= 5


	# maximum amount of children to fork
	# useful to control server load
	max_children		= 30


	# a file which will contain searchd process ID
	# used for different external automation scripts
	# MUST be present
  pid_file			= sphinx_data/log/searchd.pid


	# maximum amount of matches this daemon would ever retrieve
	# from each index and serve to client
	#
	# this parameter affects per-client memory and CPU usage
	# (16+ bytes per match) in match sorting phase; so blindly raising
	# it to 1 million is definitely NOT recommended
	#
	# starting from 0.9.7, it can be decreased on the fly through
	# the corresponding API call; increasing is prohibited to protect
	# against malicious and/or malformed requests
	#
	# default is 1000 (just like with Google)
	max_matches			= 1000
}

# --eof--
